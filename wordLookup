import os, string, nltk, re
from collections import Counter
from tabnanny import filename_only

def tokenize(text):
    text = re.sub(r"(?<![\s])([\)|\(|.|,|,\-,\"|:|;|¿|?|¡|!|“])", r" \1", text)
    text = re.sub(r"([\)|\(|.|,|,\-,\"|:|;|¿|?|¡|!|“])(?<![\s])", r"\1 ", text)
    text = re.sub(r"(ç|_)",'',text, flags=re.IGNORECASE)
    text = re.sub(r"	",' ',text, flags=re.IGNORECASE)
    text = re.sub(r"^ ", "", text, flags=re.IGNORECASE)
    return text

def flatten_list(_2d_list):
    flat_list = []
    # Iterate through the outer list
    for element in _2d_list:
        if type(element) is list:
            # If the element is of type list, iterate through the sublist
            for item in element:
                flat_list.append(item)
        else:
            flat_list.append(element)
    return flat_list

# var = 0

filesText = []

directory = "text/"
for filename in os.listdir(directory):
    with open(os.path.join(directory, filename)) as myfile:
        content = myfile.read().lower()
        filesText.append(content)
    # var += 1
    # if var == 1000:
    #     break

bigrams=lambda x: Counter([i for i in nltk.bigrams(tokenize(" ".join(flatten_list(x))).split())])

examples = (bigrams(filesText))

with open("test.txt","w") as file:
    output = []
    for i in examples.most_common():
        if i[0][0] == "transgender":
            output.append(i)
    file.write(str(output))
        

# for i in examples.most_common():
#     if i[0][0] == "trans":
#         print(i)
        